{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import *\n",
    "import glob\n",
    "import scipy \n",
    "from datetime import datetime as dt\n",
    "import sklearn\n",
    "sns.style = 'darkgrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_variance(gps_df):\n",
    "    \"\"\"\n",
    "    returns the location variance of the gps dataframe, which is log(variance of latitiude squared plus variance of \n",
    "    longitude squared)\n",
    "    \"\"\"\n",
    "    num =  gps_df['lon'].var() + gps_df['lat'].var()\n",
    "    return log(num)\n",
    "\n",
    "def num_changes(wifi_locations): \n",
    "    changes = -1\n",
    "    previous = None\n",
    "    \n",
    "    for location in wifi_locations['location'].values: \n",
    "        if location != previous:\n",
    "            changes += 1\n",
    "            previous = location\n",
    "        else: \n",
    "            continue\n",
    "            \n",
    "    return changes\n",
    "\n",
    "def time_in_range(start, end, x):\n",
    "    \"\"\"Return true if x is in the range [start, end]\"\"\"\n",
    "    if start <= end:\n",
    "        return start <= x <= end\n",
    "    else:\n",
    "        return start <= x or x <= end\n",
    "    \n",
    "def load_activity(uid): \n",
    "    # load activity data\n",
    "    activity = pd.read_csv('dataset/sensing/activity/activity_' + uid + '.csv')\n",
    "    activity['time'] = pd.to_datetime(activity['timestamp'], unit = 's') \n",
    "    activity['day'] = activity['time'].dt.dayofyear\n",
    "    activity = activity[activity[' activity inference'] != 3]\n",
    "    return activity\n",
    "\n",
    "def load_conversation(uid): \n",
    "    # load conversation data\n",
    "    conversation = pd.read_csv('dataset/sensing/conversation/conversation_' + uid + '.csv')\n",
    "    conversation['convo duration'] = conversation[' end_timestamp'] - conversation['start_timestamp']\n",
    "    conversation['day'] = pd.to_datetime(conversation['start_timestamp'], unit = 's').dt.dayofyear\n",
    "    return conversation\n",
    "\n",
    "def load_darkness(uid): \n",
    "    # load darkness data\n",
    "    darkness = pd.read_csv('dataset/sensing/dark/dark_' + uid + '.csv')\n",
    "    darkness['day'] = pd.to_datetime(darkness['start'], unit = 's').dt.dayofyear\n",
    "    darkness['duration'] = darkness['end'] - darkness['start']\n",
    "    return darkness\n",
    "\n",
    "def load_bluetooth(uid):\n",
    "    # load bluetooth data\n",
    "    bluetooth = pd.read_csv('dataset/sensing/bluetooth/bt_' + uid + '.csv')\n",
    "    bluetooth['time'] = pd.to_datetime(bluetooth['time'], unit = 's')\n",
    "    bluetooth['day'] = bluetooth['time'].dt.dayofyear\n",
    "    return bluetooth\n",
    "\n",
    "def load_gps(uid):\n",
    "    # gps data \n",
    "    gps = pd.read_csv('dataset/sensing/gps/gps_' + uid + '.csv')\n",
    "    # data is out of order, this will reformat it. \n",
    "    gps.reset_index(inplace = True)\n",
    "    gps.columns = ('timestamp', 'provider', 'network_type', 'accuracy', 'lat',\n",
    "                   'lon', 'altitude', 'bearing' ,'speed', 'travelstate', 'null')\n",
    "    gps = gps.drop(\"null\", 1)\n",
    "    gps['time'] = pd.to_datetime(gps['timestamp'], unit = 's')\n",
    "    gps['day'] = gps['time'].dt.dayofyear\n",
    "    return gps\n",
    "\n",
    "def load_wifi_locations(uid): \n",
    "    # wifi locations data\n",
    "    wifi_locations = pd.read_csv('dataset/sensing/wifi_location/wifi_location_' + uid + '.csv')\n",
    "    wifi_locations.reset_index(inplace = True)\n",
    "    wifi_locations.columns = (\"timestamp\", \"location\", \"null\")\n",
    "    wifi_locations = wifi_locations.drop(\"null\", 1)\n",
    "    wifi_locations['time'] = pd.to_datetime(wifi_locations['timestamp'], unit = 's')\n",
    "    wifi_locations['day'] = wifi_locations['time'].dt.dayofyear\n",
    "    return wifi_locations\n",
    "\n",
    "def conversation_in_range(time_interval, convo_df, start_name, end_name): \n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        time_interval -- formatted as (start time, end time, start day, end day)\n",
    "        convo_df -- a dataframe containing start and end timestamps for a duration measurement \n",
    "            (so this function can be used for darkness as well as conversation)\n",
    "        start_name -- name of the column indicating the start timestamp\n",
    "        end_name -- name of the column indicating the end timestamp. \n",
    "    outputs: \n",
    "        the total conversation duration in the time interval.\n",
    "        \n",
    "    Note -- I initially named this function for activity so the variable names reflect that, but it can be applied to\n",
    "    multiple sensor data. \n",
    "    \n",
    "    This function is is similar to the activity in range but applies to dataframes contianing durations so the approach is\n",
    "    slightly different.  \n",
    "    \"\"\"\n",
    "    # again, unpack interval. \n",
    "    start = time_interval[0]\n",
    "    end = time_interval[1]\n",
    "    start_day = time_interval[2]\n",
    "    end_day = time_interval[3]\n",
    "    \n",
    "    # look at relevant days \n",
    "    if start_day == end_day: \n",
    "        conv = convo_df[convo_df['day'] == start_day]\n",
    "    else: \n",
    "        conv = convo_df[convo_df['day'] == start_day].append(convo_df[convo_df['day'] == end_day])\n",
    "    \n",
    "    # turn the conversations into intervals. If none exist, the duration is 0. \n",
    "    try:\n",
    "        conv['interval'] = list(zip(pd.to_datetime(conv[start_name], unit = 's'), \n",
    "                                    pd.to_datetime(conv[end_name], unit = 's')))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    # this function returns the duration of conversation inside the desired interval for each time interval. \n",
    "    conv['desired duration'] = conv['interval'].apply(lambda x: conv_range(start, end, x))\n",
    "    conv = conv.dropna()\n",
    "    \n",
    "    # return the sum of all desired intervals. \n",
    "    return conv['desired duration'].sum()\n",
    "\n",
    "def conv_range(start, end, conv_interval): \n",
    "    \"\"\"\n",
    "    returns the amount of seconds of conversation are in the interval (start, end)\n",
    "    \"\"\"\n",
    "    conv_start = conv_interval[0]\n",
    "    conv_end = conv_interval[1]\n",
    "    \n",
    "    if conv_end < start: \n",
    "        return np.nan\n",
    "    \n",
    "    elif conv_start > end:\n",
    "        return np.nan\n",
    "    \n",
    "    elif conv_start >= start and conv_end >= end:\n",
    "        return end - conv_start \n",
    "    \n",
    "    elif conv_start <= start and conv_end <= end:\n",
    "        return conv_end - start\n",
    "    \n",
    "    elif conv_start >= start and conv_end <= end:\n",
    "        return conv_end - conv_start\n",
    "    \n",
    "    elif conv_start <= start and conv_end >= end:\n",
    "        return end - start\n",
    "    \n",
    "def convert_timedeltas(x): \n",
    "    \"\"\"\n",
    "    converts timedeltas to seconds, leaves any numbers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return x.seconds\n",
    "    except:\n",
    "        return x \n",
    "    \n",
    "def activity_in_range(time_interval, activity_df, func = 'act'): \n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        time_interval -- formatted as (start time, end time, start day, end day)\n",
    "        activity_df -- dataframe for a single user. \n",
    "    outputs: \n",
    "        the mean activity inference in the time interval.\n",
    "        \n",
    "    Note: the activity dataframe and variable names imply \n",
    "    \"\"\"\n",
    "    \n",
    "    # unpack the values from the time interval\n",
    "    start = time_interval[0]\n",
    "    end = time_interval[1]\n",
    "    start_day = time_interval[2]\n",
    "    end_day = time_interval[3]\n",
    "    \n",
    "    # only look at relevant days to say runtime\n",
    "    if start_day == end_day: \n",
    "        activity = activity_df[activity_df['day'] == start_day]\n",
    "    else: \n",
    "        activity = activity_df[activity_df['day'] == start_day].append(activity_df[activity_df['day'] == end_day])\n",
    "        \n",
    "    # this try except loop takes care of the case where the activity data is an empty dataframe, so we return Nan \n",
    "    try: \n",
    "        ### these cases are different for different func inputs so this function can be extensible. \n",
    "        \n",
    "        # in this case, we are looking at activity and taking the mean\n",
    "        if func == 'act':\n",
    "            return activity[activity['time'].apply(lambda x: time_in_range(start, end, x))][' activity inference'].sum()\n",
    "        elif func == 'all_act': \n",
    "            print(activity[activity['time'].apply(lambda x: time_in_range(start, end, x))][' activity inference'].values)\n",
    "            return activity[activity['time'].apply(lambda x: time_in_range(start, end, x))][' activity inference'].values\n",
    "        # in this case, we are looking at bluetooth and take the count\n",
    "        elif func == 'count':\n",
    "            return activity[activity['time'].apply(lambda x: time_in_range(start, end, x))].shape[0]\n",
    "        # in this case we apply the location variance function \n",
    "        elif func == 'location variance': \n",
    "            return location_variance(activity[activity['time'].apply(lambda x: time_in_range(start, end, x))])\n",
    "        elif func == 'location changes': \n",
    "            return num_changes(activity[activity['time'].apply(lambda x: time_in_range(start, end, x))])\n",
    "    except:\n",
    "        # if we find none in count, we return 0. If not, there is no data/average from there so return Nan. \n",
    "        if func == 'count': \n",
    "            return 0\n",
    "        return np.nan\n",
    "\n",
    "#This function returns a new dataframe with all of the activity durations for a particular student throughout the term.\n",
    "#Only activities longer than 1 minute were considered.\n",
    "#At the end, we dediced to use total activity duration (sum of activity durations per day) for our model\n",
    "\n",
    "def activity_analysis(uid):\n",
    "    activity = pd.read_csv('dataset/sensing/activity/activity_' + uid + '.csv')\n",
    "    activity = activity[activity[' activity inference'] !=3]\n",
    "    activity = activity.reset_index()\n",
    "    #Change the path as needed when running the files on your computer.\n",
    "    activity['day'] = pd.to_datetime(activity['timestamp'], unit = 's').dt.dayofyear\n",
    "    daily_activity = activity.groupby('day').mean()\n",
    "    def shift_counter_activity(data):\n",
    "        shift_num = 0\n",
    "        list_shift_num = []\n",
    "        list_time = []\n",
    "        list_day = []\n",
    "        for i in range(0, len(data)):\n",
    "            if data[' activity inference'][i] != 0:\n",
    "                try: \n",
    "                    if data[' activity inference'][i+1] != 0 and (data.index[i]+1) == data.index[i+1]:\n",
    "                        shift_num += 1\n",
    "                    else:\n",
    "                        list_shift_num.append(shift_num)\n",
    "                        shift_num = 0\n",
    "                except:\n",
    "                    list_shift_num.append(shift_num)\n",
    "                    shift_num = 0\n",
    "        return list_shift_num\n",
    "    activity_shifts = shift_counter_activity(activity)\n",
    "    edited_act = activity[activity[' activity inference'] !=0]\n",
    "    edited_act = edited_act.reset_index()\n",
    "    def shifts_only(list1):\n",
    "        shifts_only_list = []\n",
    "        for i in list1:\n",
    "            if i != 0:\n",
    "                shifts_only_list.append(i)\n",
    "        return shifts_only_list\n",
    "    new_activity_shifts = shifts_only(activity_shifts)\n",
    "    def get_sums(list1):\n",
    "        list_sums_b = []\n",
    "        for i in range(0,len(list1)+1):\n",
    "            new_list = list1[:i]\n",
    "            sums = sum(new_list)\n",
    "            list_sums_b.append(sums)\n",
    "        return list_sums_b\n",
    "    list_sums_before_activity = get_sums(activity_shifts)\n",
    "    def activity_dur(list_shift_num, data):\n",
    "        time_deltas = []\n",
    "        day = []\n",
    "        start_time = []\n",
    "        for i in range(0, len(list_shift_num)):\n",
    "            if i == 0:\n",
    "                time_deltas.append(data['timestamp'][list_shift_num[i]] - data['timestamp'][0])\n",
    "                day.append(data.day[list_shift_num[i]+i+list_sums_before_activity[i]])\n",
    "                start_time.append(data.timestamp[list_shift_num[i]+i+list_sums_before_activity[i]])\n",
    "            elif i != 0:\n",
    "                time_deltas.append(data['timestamp'][list_shift_num[i]+i+list_sums_before_activity[i]] - data['timestamp'][list_sums_before_activity[i]+i])\n",
    "                day.append(data.day[list_shift_num[i]+i+list_sums_before_activity[i]])\n",
    "                start_time.append(data.timestamp[list_shift_num[i]+i+list_sums_before_activity[i]])\n",
    "        dataframe = pd.DataFrame({'Time Delta': time_deltas, 'day': day, 'Start Time': start_time})\n",
    "        return dataframe\n",
    "    activity_dur_df = activity_dur(activity_shifts, edited_act)\n",
    "    activity_dur_df['end_time'] = activity_dur_df['Start Time'] + activity_dur_df['Time Delta']\n",
    "    activity_dur_df['start_day'] = pd.to_datetime(activity_dur_df['Start Time'], unit='s').dt.dayofyear\n",
    "    activity_dur_df['end_day'] = pd.to_datetime(activity_dur_df['end_time'], unit='s').dt.dayofyear\n",
    "    activity_dur_df = activity_dur_df.rename(columns={'Start Time': 'start_time'})\n",
    "    #activity_dur_df = activity_dur_df[activity_dur_df['Time Delta'] >= 60]\n",
    "    activity_dur_day = activity_dur_df.groupby('day')['Time Delta'].sum()\n",
    "    return activity_dur_df\n",
    "\n",
    "def deadlines_processing():\n",
    "    data = pd.read_csv('dataset/education/deadlines.csv')\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    data = data.T\n",
    "    old_names = list(data.columns)\n",
    "    new_names = data.iloc[0]\n",
    "    data.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    data = data.drop(['uid'])\n",
    "    data['doy'] = pd.to_datetime(data.index)\n",
    "    data['doy'] = data['doy'].dt.dayofyear\n",
    "    return data\n",
    "\n",
    "def epoch(hour): \n",
    "    if hour >= 18: \n",
    "        return 'evening'\n",
    "    elif hour < 10: \n",
    "        return 'night'\n",
    "    else:\n",
    "        return 'day'\n",
    "    \n",
    "def midterm(day): \n",
    "    if day < 21 + 86: \n",
    "        return 'pre midterm'\n",
    "    elif (21 + 86) <= day <= (35 + 86):\n",
    "        return 'in midterm'\n",
    "    elif (35 + 86) < day:\n",
    "        return 'post midterm'\n",
    "    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def ema_intervals_data(uid, window, ema_name, desired_column, before=False): \n",
    "    \"\"\"\n",
    "    inputs: uid -- user id \n",
    "            window -- the frame of time (in hours) of how long the interval of sensor collection around each EMA should be. \n",
    "    \n",
    "    Finds desired sensor data within that window of time before and after the EMA. \n",
    "    \n",
    "    Returns: a dataframe containing stress level and desired feature information for each stress response. If the\n",
    "    dataframe has less than 50 elements returns none (we assume there isn't enough data with less than 50 elements). \n",
    "    \"\"\"\n",
    "    data = process_ema(uid, ema_name, desired_column)\n",
    "    \n",
    "    # define the window of time we want to look at for each stress answer. \n",
    "    data['start_time'] = data['resp_time'] - pd.to_timedelta(window, unit = 'h')\n",
    "    if before is True: \n",
    "        data['end_time'] = data['resp_time']\n",
    "    else: \n",
    "        data['end_time'] = data['resp_time'] + pd.to_timedelta(window, unit = 'h')\n",
    "    \n",
    "    data['hour'] = data['resp_time'].dt.hour\n",
    "    data['epoch'] = data['hour'].apply(epoch)\n",
    "    data = data.join(pd.get_dummies(data['epoch']))\n",
    "    \n",
    "    # this will reduce runtime by only looking at sensor data from that day then applying our interval function to it. \n",
    "    data['start_day'] = data['start_time'].dt.dayofyear\n",
    "    data['end_day'] = data['end_time'].dt.dayofyear\n",
    "    data['doy'] = data['resp_time'].dt.dayofyear\n",
    "    \n",
    "    data['dow'] = data['resp_time'].dt.dayofweek\n",
    "    data = data.join(pd.get_dummies(data['dow']))\n",
    "    data = data.rename(columns={0: 'Monday', \n",
    "                                1: 'Tuesday', \n",
    "                                2: 'Wednesday', \n",
    "                                3: 'Thursday', \n",
    "                                4: 'Friday',\n",
    "                                5: 'Saturday',\n",
    "                                6: 'Sunday'})\n",
    "    \n",
    "    data['midterm'] = data['doy'].apply(midterm)\n",
    "    data = data.join(pd.get_dummies(data['midterm']))\n",
    "    \n",
    "    # the time interval is just a tuple of (start time, end time)\n",
    "    # in the future, we will apply functions to the interval using other dataframes to return desired columns inside\n",
    "    # the interval\n",
    "    data['interval'] = tuple(zip(data['start_time'], data['end_time'], data['start_day'], data['end_day']))\n",
    "    \n",
    "    # load activity data\n",
    "    activity = activity_analysis(uid)\n",
    "    data['activity dur'] = data['interval'].apply(lambda x: conversation_in_range(x, activity, \n",
    "                                                                           'start_time', 'end_time'))\n",
    "    data['activity dur'] = data['activity dur'].apply(convert_timedeltas)\n",
    "    \n",
    "    # this will return the total conversation duration for each interval\n",
    "    conversation = load_conversation(uid)\n",
    "    data['conversation dur'] = data['interval'].apply(lambda x: conversation_in_range(x, conversation, \n",
    "                                                                           'start_timestamp', ' end_timestamp'))\n",
    "    data['conversation dur'] = data['conversation dur'].apply(convert_timedeltas)\n",
    "    \n",
    "    # find the total darkness duration for each interval\n",
    "    darkness = load_darkness(uid)\n",
    "    data['darkness dur'] = data['interval'].apply(lambda x: conversation_in_range(x, darkness, 'start', 'end'))\n",
    "    data['darkness dur'] = data['darkness dur'].apply(convert_timedeltas)\n",
    "    \n",
    "    \n",
    "    # find the number of bluetooth colocations in each interval\n",
    "    bluetooth = load_bluetooth(uid)\n",
    "    data['bluetooth colocations'] = data['interval'].apply(lambda x: activity_in_range(x, bluetooth, 'count'))\n",
    "    \n",
    "    \n",
    "    # find the location variance in each stress interval. \n",
    "    gps = load_gps(uid)\n",
    "    data['location variance'] = data['interval'].apply(lambda x: activity_in_range(x, gps, 'location variance'))\n",
    "    \n",
    "    # wifi locations\n",
    "    wifi_locations = load_wifi_locations(uid)\n",
    "    data['location changes'] = data['interval'].apply(lambda x: activity_in_range(x, wifi_locations, 'location changes'))\n",
    "    \n",
    "    #load deadlines data.\n",
    "    deadlines = deadlines_processing()\n",
    "    #deadlines = deadlines[deadlines['doy' == data.start_day]]\n",
    "    deadlines = deadlines[[uid, 'doy']]\n",
    "    data = pd.merge(data, deadlines, on='doy', how='inner')\n",
    "    data = data.rename(columns={uid: 'deadlines'})\n",
    "  \n",
    "    # drop Nan values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # only use these features if we have over 50 datapoints\n",
    "    if data.shape[0] < 20: \n",
    "        return None\n",
    "    \n",
    "    data.sort_values(by=['resp_time'], inplace=True)\n",
    "\n",
    "    \n",
    "    # return relevant columns. \n",
    "    return data[['resp_time', desired_column, 'location changes', 'activity dur',\n",
    "                'conversation dur', 'darkness dur', 'bluetooth colocations', 'location variance', 'deadlines', \n",
    "                'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', \n",
    "                'day', 'evening', 'night', 'pre midterm', 'in midterm', 'post midterm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lowell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Lowell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['u00'] not in index\"\n",
      "u00\n",
      "\"['u00'] not in index\"\n",
      "u00\n",
      "\"['u00'] not in index\"\n",
      "u00\n",
      "\"['u00'] not in index\"\n",
      "u00\n",
      "\"['u00'] not in index\"\n",
      "u00\n",
      "\"['u00'] not in index\"\n",
      "u00\n",
      "\"['Monday', 'in midterm', 'Tuesday', 'post midterm'] not in index\"\n",
      "u05\n",
      "\"['Monday', 'in midterm', 'Tuesday', 'post midterm'] not in index\"\n",
      "u05\n",
      "\"['Monday', 'in midterm', 'Tuesday', 'post midterm'] not in index\"\n",
      "u05\n",
      "\"['Monday', 'in midterm', 'Tuesday', 'post midterm'] not in index\"\n",
      "u05\n",
      "\"['Monday', 'in midterm', 'Tuesday', 'post midterm'] not in index\"\n",
      "u05\n",
      "\"['Monday', 'in midterm', 'Tuesday', 'post midterm'] not in index\"\n",
      "u05\n",
      "\"['in midterm', 'post midterm'] not in index\"\n",
      "u20\n",
      "\"['in midterm', 'post midterm'] not in index\"\n",
      "u20\n",
      "\"['in midterm', 'post midterm'] not in index\"\n",
      "u20\n",
      "\"['in midterm', 'post midterm'] not in index\"\n",
      "u20\n",
      "\"['in midterm', 'post midterm'] not in index\"\n",
      "u20\n",
      "\"['in midterm', 'post midterm'] not in index\"\n",
      "u20\n",
      "\"['post midterm'] not in index\"\n",
      "u23\n",
      "\"['post midterm'] not in index\"\n",
      "u23\n",
      "\"['post midterm'] not in index\"\n",
      "u23\n",
      "\"['post midterm'] not in index\"\n",
      "u23\n",
      "\"['post midterm'] not in index\"\n",
      "u23\n",
      "\"['post midterm'] not in index\"\n",
      "u23\n",
      "\"['post midterm'] not in index\"\n",
      "u31\n",
      "\"['post midterm'] not in index\"\n",
      "u31\n",
      "\"['post midterm'] not in index\"\n",
      "u31\n",
      "\"['post midterm'] not in index\"\n",
      "u31\n",
      "\"['post midterm'] not in index\"\n",
      "u31\n",
      "\"['post midterm'] not in index\"\n",
      "u31\n",
      "\"['u36'] not in index\"\n",
      "u36\n",
      "\"['u36'] not in index\"\n",
      "u36\n",
      "\"['u36'] not in index\"\n",
      "u36\n",
      "\"['u36'] not in index\"\n",
      "u36\n",
      "\"['u36'] not in index\"\n",
      "u36\n",
      "\"['u36'] not in index\"\n",
      "u36\n",
      "\"['u39'] not in index\"\n",
      "u39\n",
      "\"['u39'] not in index\"\n",
      "u39\n",
      "\"['u39'] not in index\"\n",
      "u39\n",
      "\"['u39'] not in index\"\n",
      "u39\n",
      "\"['u39'] not in index\"\n",
      "u39\n",
      "\"['u39'] not in index\"\n",
      "u39\n",
      "\"['u43'] not in index\"\n",
      "u43\n",
      "\"['u43'] not in index\"\n",
      "u43\n",
      "\"['u43'] not in index\"\n",
      "u43\n",
      "\"['u43'] not in index\"\n",
      "u43\n",
      "\"['u43'] not in index\"\n",
      "u43\n",
      "\"['u43'] not in index\"\n",
      "u43\n",
      "\"['in midterm'] not in index\"\n",
      "u50\n",
      "\"['in midterm'] not in index\"\n",
      "u50\n",
      "\"['in midterm'] not in index\"\n",
      "u50\n",
      "\"['in midterm'] not in index\"\n",
      "u50\n",
      "\"['in midterm'] not in index\"\n",
      "u50\n",
      "\"['in midterm'] not in index\"\n",
      "u50\n",
      "\"['u56'] not in index\"\n",
      "u56\n",
      "\"['u56'] not in index\"\n",
      "u56\n",
      "\"['u56'] not in index\"\n",
      "u56\n",
      "\"['u56'] not in index\"\n",
      "u56\n",
      "\"['u56'] not in index\"\n",
      "u56\n",
      "\"['u56'] not in index\"\n",
      "u56\n"
     ]
    }
   ],
   "source": [
    "def save_aggregated_data(windows, ema, desired_column, before=False):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    ema_files = glob.glob('dataset/EMA/response/' + ema + '/' + ema + '_*.json')\n",
    "    uid_start = len('dataset/EMA/response/' + ema + '/' + ema + '_')\n",
    "    \n",
    "    # loops through all the files and averages the feature importance lists\n",
    "    for file in ema_files: \n",
    "        uid = file[uid_start:uid_start+3]\n",
    "        for window in windows: \n",
    "            try: \n",
    "                data = ema_intervals_data(uid, window, ema, desired_column, before)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(uid)\n",
    "                continue\n",
    "            if data is None:\n",
    "                continue\n",
    "            data['uid'] = uid\n",
    "            data['window'] = window\n",
    "            df = df.append(data)\n",
    "            \n",
    "    df.to_csv('ema_data\\{}, before = {}.csv'.format(ema, before))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "x = save_aggregated_data([2, 4, 6, 8, 10, 12], 'PAM', 'picture_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>picture_idx</th>\n",
       "      <th>location changes</th>\n",
       "      <th>activity dur</th>\n",
       "      <th>conversation dur</th>\n",
       "      <th>darkness dur</th>\n",
       "      <th>bluetooth colocations</th>\n",
       "      <th>location variance</th>\n",
       "      <th>deadlines</th>\n",
       "      <th>...</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>pre midterm</th>\n",
       "      <th>in midterm</th>\n",
       "      <th>post midterm</th>\n",
       "      <th>uid</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2013-03-27 04:45:45</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>7633.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.356564</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2013-03-27 04:45:46</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>7634.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.356564</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-03-27 04:45:47</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>7635.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.356564</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-03-27 07:03:27</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.499824</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>2013-03-27 07:50:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.613747</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42337</th>\n",
       "      <td>417</td>\n",
       "      <td>2013-05-31 09:55:34</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>11232.0</td>\n",
       "      <td>28811.0</td>\n",
       "      <td>25177.0</td>\n",
       "      <td>175</td>\n",
       "      <td>-1.381905</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42338</th>\n",
       "      <td>414</td>\n",
       "      <td>2013-05-31 18:07:45</td>\n",
       "      <td>4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>15509.0</td>\n",
       "      <td>21340.0</td>\n",
       "      <td>30135.0</td>\n",
       "      <td>123</td>\n",
       "      <td>0.668664</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42339</th>\n",
       "      <td>415</td>\n",
       "      <td>2013-05-31 20:00:38</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>15499.0</td>\n",
       "      <td>21269.0</td>\n",
       "      <td>27664.0</td>\n",
       "      <td>112</td>\n",
       "      <td>0.708999</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42340</th>\n",
       "      <td>419</td>\n",
       "      <td>2013-06-01 00:23:29</td>\n",
       "      <td>4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>15499.0</td>\n",
       "      <td>21269.0</td>\n",
       "      <td>11893.0</td>\n",
       "      <td>85</td>\n",
       "      <td>0.774796</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42341</th>\n",
       "      <td>421</td>\n",
       "      <td>2013-06-01 04:03:15</td>\n",
       "      <td>2</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15072.0</td>\n",
       "      <td>18923.0</td>\n",
       "      <td>4958.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0.703076</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42342 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            resp_time  picture_idx  location changes  \\\n",
       "0              10  2013-03-27 04:45:45            4              10.0   \n",
       "1               9  2013-03-27 04:45:46            2              10.0   \n",
       "2               5  2013-03-27 04:45:47            1              10.0   \n",
       "3              11  2013-03-27 07:03:27            3              10.0   \n",
       "4              13  2013-03-27 07:50:00            1              13.0   \n",
       "...           ...                  ...          ...               ...   \n",
       "42337         417  2013-05-31 09:55:34            4             140.0   \n",
       "42338         414  2013-05-31 18:07:45            4              62.0   \n",
       "42339         415  2013-05-31 20:00:38            1              62.0   \n",
       "42340         419  2013-06-01 00:23:29            4              62.0   \n",
       "42341         421  2013-06-01 04:03:15            2              49.0   \n",
       "\n",
       "       activity dur  conversation dur  darkness dur  bluetooth colocations  \\\n",
       "0               0.0             413.0        7633.0                      3   \n",
       "1               0.0             413.0        7634.0                      3   \n",
       "2               0.0             413.0        7635.0                      3   \n",
       "3               0.0               0.0       14400.0                      1   \n",
       "4               0.0               0.0       14400.0                      0   \n",
       "...             ...               ...           ...                    ...   \n",
       "42337       11232.0           28811.0       25177.0                    175   \n",
       "42338       15509.0           21340.0       30135.0                    123   \n",
       "42339       15499.0           21269.0       27664.0                    112   \n",
       "42340       15499.0           21269.0       11893.0                     85   \n",
       "42341       15072.0           18923.0        4958.0                     65   \n",
       "\n",
       "       location variance  deadlines  ...  Saturday  Sunday  day  evening  \\\n",
       "0             -17.356564          0  ...         0       0    0        0   \n",
       "1             -17.356564          0  ...         0       0    0        0   \n",
       "2             -17.356564          0  ...         0       0    0        0   \n",
       "3             -17.499824          0  ...         0       0    0        0   \n",
       "4             -17.613747          0  ...         0       0    0        0   \n",
       "...                  ...        ...  ...       ...     ...  ...      ...   \n",
       "42337          -1.381905          0  ...         0       0    0        0   \n",
       "42338           0.668664          0  ...         0       0    0        1   \n",
       "42339           0.708999          0  ...         0       0    0        1   \n",
       "42340           0.774796          0  ...         1       0    0        0   \n",
       "42341           0.703076          0  ...         1       0    0        0   \n",
       "\n",
       "       night  pre midterm  in midterm  post midterm  uid  window  \n",
       "0          1            1           0             0  u01       2  \n",
       "1          1            1           0             0  u01       2  \n",
       "2          1            1           0             0  u01       2  \n",
       "3          1            1           0             0  u01       2  \n",
       "4          1            1           0             0  u01       2  \n",
       "...      ...          ...         ...           ...  ...     ...  \n",
       "42337      1            0           0             1  u59      12  \n",
       "42338      0            0           0             1  u59      12  \n",
       "42339      0            0           0             1  u59      12  \n",
       "42340      1            0           0             1  u59      12  \n",
       "42341      1            0           0             1  u59      12  \n",
       "\n",
       "[42342 rows x 25 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ema_data/PAM, before = False.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stress(level):\n",
    "    \"\"\"\n",
    "    converts input stress level from the scale above into a more usable scale with 1 being feeling great \n",
    "    and 5 being stressed out.\n",
    "    \"\"\"\n",
    "    # little stress = 3/5 stressed\n",
    "    if level == 1: \n",
    "        return 3\n",
    "    # definitely stressed = 4/5\n",
    "    if level == 2:\n",
    "        return 4\n",
    "    # stressed out = 5/5\n",
    "    if level == 3:\n",
    "        return 5\n",
    "    # feeling good = 2/5\n",
    "    if level == 4: \n",
    "        return 2\n",
    "    # feeling great = 1/5 \n",
    "    if level == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_PAM(level):\n",
    "    \"\"\"\n",
    "    assigns PAM picture_idx levels to four ranges (four quadrants):\n",
    "    Quadrant 1: negative valence and low arousal; Quadrant 2: negative valence and high arousal; \n",
    "    Quadrant 3: positive valence and low arousal; Quadrant 4: positive valence and high arousal\n",
    "    \"\"\"\n",
    "    quadrant_1 = list(range(1,5))\n",
    "    quadrant_2 = list(range(5,9))\n",
    "    quadrant_3 = list(range(9,13))\n",
    "    quadrant_4 = list(range(13,17))\n",
    "    \n",
    "    if level in quadrant_1:\n",
    "        return 1\n",
    "    if level in quadrant_2: \n",
    "        return 2\n",
    "    if level in quadrant_3: \n",
    "        return 3\n",
    "    if level in quadrant_4: \n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ema(uid, ema_name, desired_column): \n",
    "    \"\"\"\n",
    "    input: uid for which we want to process the EMA. \n",
    "           the name of the ema we want to process\n",
    "           the column that represents the scoring area of interest for the particular ema. \n",
    "    output: \n",
    "        a dataframe containing the response time and score for each ema response. \n",
    "    \"\"\"\n",
    "    \n",
    "    ema = pd.read_json('dataset/EMA/response/{}/{}_{}.json'.format(ema_name, ema_name, uid))\n",
    "    \n",
    "    # this takes the desired values that could be in the \"null\" column and puts them into the desired colum \n",
    "    try: \n",
    "        ema[desired_column] = ema[desired_column].where(np.isfinite, ema.null)\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    # get rid of the non-numeric answers from the null column.  \n",
    "    ema[desired_column] = pd.to_numeric(ema[desired_column], errors='coerce')\n",
    "    \n",
    "    ema = ema[['resp_time', desired_column]]\n",
    "    ema = ema.dropna()\n",
    "    \n",
    "    if ema_name == 'stress' or ema_name == 'Stress': \n",
    "        ema['level'] = ema['level'].apply(convert_stress)\n",
    "        \n",
    "    if ema_name == 'PAM':\n",
    "        ema['picture_idx'] = ema['picture_idx'].apply(convert_PAM)\n",
    "    \n",
    "    return ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skewness(uid, ema, desired_column, start, stop, step = 1): \n",
    "    \n",
    "    x = process_ema(uid, ema, desired_column)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(start, stop + step, step): \n",
    "        val = x[x[desired_column] == i].shape[0]\n",
    "        df['response {}'.format(i)] = [val]\n",
    "        \n",
    "    df['uid'] = uid \n",
    "    \n",
    "    df['total'] = x.shape[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response 1</th>\n",
       "      <th>response 2</th>\n",
       "      <th>response 3</th>\n",
       "      <th>response 4</th>\n",
       "      <th>response 5</th>\n",
       "      <th>uid</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>u00</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response 1  response 2  response 3  response 4  response 5  uid  total\n",
       "0           3          16          32          14          11  u00     76"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_skewness('u00', 'stress', 'level', 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_skewness(ema, desired_column, start, stop, step=1): \n",
    "    \n",
    "    total_data = pd.DataFrame()\n",
    "    \n",
    "    ema_files = glob.glob('dataset/EMA/response/' + ema + '/' + ema + '_*.json')\n",
    "    uid_start = len('dataset/EMA/response/' + ema + '/' + ema + '_')\n",
    "    # loops through all the files and averages the feature importance lists\n",
    "    for file in ema_files: \n",
    "        uid = file[uid_start:uid_start+3]\n",
    "        try: \n",
    "            data = get_skewness(uid, ema, desired_column, start, stop, step)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        total_data = total_data.append(data, ignore_index = True)\n",
    "\n",
    "    \n",
    "    total_data.loc['Total'] = total_data.sum(numeric_only = True)\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response 1</th>\n",
       "      <th>response 2</th>\n",
       "      <th>response 3</th>\n",
       "      <th>response 4</th>\n",
       "      <th>response 5</th>\n",
       "      <th>uid</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>u00</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>u08</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>u10</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>u16</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>u19</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>u43</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>u44</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>u59</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>125.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2288.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       response 1  response 2  response 3  response 4  response 5  uid   total\n",
       "0             3.0        16.0        32.0        14.0        11.0  u00    76.0\n",
       "7             0.0         3.0        40.0        31.0        17.0  u08    91.0\n",
       "9             3.0        42.0        50.0        12.0         1.0  u10   108.0\n",
       "13            0.0        12.0        41.0        25.0        32.0  u16   110.0\n",
       "16            1.0        10.0        37.0        31.0        13.0  u19    92.0\n",
       "33            1.0         6.0        44.0         5.0        23.0  u43    79.0\n",
       "34            2.0        23.0        52.0         7.0         6.0  u44    90.0\n",
       "47           20.0        89.0       133.0        20.0         7.0  u59   269.0\n",
       "Total       125.0       472.0      1013.0       369.0       309.0  NaN  2288.0"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=get_all_skewness('stress', 'level', 1, 5)\n",
    "y[y['total'] > 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response 1</th>\n",
       "      <th>response 2</th>\n",
       "      <th>response 3</th>\n",
       "      <th>response 4</th>\n",
       "      <th>uid</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>u00</td>\n",
       "      <td>390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>u01</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>u02</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>u03</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>u04</td>\n",
       "      <td>218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>u05</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>u07</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>u08</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>u09</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>u10</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>u12</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>u13</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>u14</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>u15</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>41.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>u16</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>u17</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>u18</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>u19</td>\n",
       "      <td>384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>u20</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>76.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>u22</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>52.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>u23</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>43.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>u24</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>u25</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>u27</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>u30</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>u31</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>u32</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>58.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>u33</td>\n",
       "      <td>292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>u34</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>26.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>u35</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>21.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>u36</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>u39</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>u41</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>24.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>u42</td>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>24.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>u43</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>38.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>u44</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>22.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>u45</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>u46</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>u47</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>34.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>u49</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>u50</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>61.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>u51</td>\n",
       "      <td>298.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>33.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>u52</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>u53</td>\n",
       "      <td>218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>29.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>u54</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>u56</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>54.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>u57</td>\n",
       "      <td>377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>133.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>u58</td>\n",
       "      <td>327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>u59</td>\n",
       "      <td>437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>1378.0</td>\n",
       "      <td>3317.0</td>\n",
       "      <td>2181.0</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9040.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       response 1  response 2  response 3  response 4  uid   total\n",
       "0            41.0       192.0        83.0        74.0  u00   390.0\n",
       "1            15.0        36.0        38.0        51.0  u01   140.0\n",
       "2            32.0        77.0        71.0        42.0  u02   222.0\n",
       "3            30.0        36.0        20.0        24.0  u03   110.0\n",
       "4            38.0        65.0        65.0        50.0  u04   218.0\n",
       "5             3.0        13.0         6.0         8.0  u05    30.0\n",
       "6            11.0        55.0        37.0        44.0  u07   147.0\n",
       "7            37.0        70.0        74.0        61.0  u08   242.0\n",
       "8             3.0         6.0         4.0         8.0  u09    21.0\n",
       "9            15.0       105.0       119.0        74.0  u10   313.0\n",
       "10           26.0        81.0        24.0        70.0  u12   201.0\n",
       "11            2.0         4.0         0.0         2.0  u13     8.0\n",
       "12           30.0        98.0        52.0        18.0  u14   198.0\n",
       "13            6.0        20.0        11.0        16.0  u15    53.0\n",
       "14           41.0       102.0       117.0        99.0  u16   359.0\n",
       "15           13.0        47.0        28.0       127.0  u17   215.0\n",
       "16            7.0        30.0        58.0        64.0  u18   159.0\n",
       "17           16.0        42.0       157.0       169.0  u19   384.0\n",
       "18           17.0        21.0        11.0         2.0  u20    51.0\n",
       "19           76.0        65.0        51.0        18.0  u22   210.0\n",
       "20           52.0        92.0        14.0         1.0  u23   159.0\n",
       "21           43.0        70.0        22.0        32.0  u24   167.0\n",
       "22           14.0        35.0        12.0        35.0  u25    96.0\n",
       "23           23.0        60.0        44.0        55.0  u27   182.0\n",
       "24           13.0        45.0        66.0        23.0  u30   147.0\n",
       "25           16.0        24.0        20.0        22.0  u31    82.0\n",
       "26           14.0        99.0        61.0        66.0  u32   240.0\n",
       "27           58.0        97.0        60.0        77.0  u33   292.0\n",
       "28           16.0        14.0        10.0        14.0  u34    54.0\n",
       "29           26.0        76.0        77.0        26.0  u35   205.0\n",
       "30           21.0       137.0        59.0        43.0  u36   260.0\n",
       "31            3.0         7.0         1.0         3.0  u39    14.0\n",
       "32           15.0        30.0        10.0        27.0  u41    82.0\n",
       "33           24.0        96.0        12.0        11.0  u42   143.0\n",
       "34           24.0       117.0        45.0        43.0  u43   229.0\n",
       "35           38.0        82.0        53.0        57.0  u44   230.0\n",
       "36           22.0        52.0        17.0        15.0  u45   106.0\n",
       "37           29.0        88.0        38.0        40.0  u46   195.0\n",
       "38           30.0        47.0        10.0        19.0  u47   106.0\n",
       "39           34.0        81.0        79.0        49.0  u49   243.0\n",
       "40            8.0        28.0         8.0        13.0  u50    57.0\n",
       "41           61.0       144.0        44.0        49.0  u51   298.0\n",
       "42           33.0        98.0        62.0        31.0  u52   224.0\n",
       "43           29.0        38.0        71.0        80.0  u53   218.0\n",
       "44           29.0        32.0        24.0        19.0  u54   104.0\n",
       "45            9.0        29.0        19.0        38.0  u56    95.0\n",
       "46           54.0       109.0        95.0       119.0  u57   377.0\n",
       "47          133.0       139.0        42.0        13.0  u58   327.0\n",
       "48           48.0       186.0        80.0       123.0  u59   437.0\n",
       "Total      1378.0      3317.0      2181.0      2164.0  NaN  9040.0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = get_all_skewness('PAM', 'picture_idx', 1, 4)\n",
    "x\n",
    "#x[['response 1, response 2', 'response 3', 'response 4']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_list = list(x[x['total'] > 200].uid.dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response 1</th>\n",
       "      <th>response 2</th>\n",
       "      <th>response 3</th>\n",
       "      <th>response 4</th>\n",
       "      <th>uid</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>186</td>\n",
       "      <td>80</td>\n",
       "      <td>123</td>\n",
       "      <td>u59</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response 1  response 2  response 3  response 4  uid  total\n",
       "0          48         186          80         123  u59    437"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u59_pam = process_ema('u59', 'PAM', 'picture_idx')\n",
    "get_skewness('u59', 'PAM', 'picture_idx', 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   et score  rf score  corrupted et score  corrupted rf score\n",
      "0  0.220290  0.214493            0.269565            0.252174\n",
      "1  0.231884  0.217391            0.234783            0.231884\n",
      "2  0.217391  0.214493            0.286957            0.292754\n",
      "3  0.211594  0.214493            0.257971            0.252174\n",
      "4  0.214493  0.214493            0.249275            0.231884\n",
      "5  0.220290  0.217391            0.231884            0.228986\n",
      "6  0.223188  0.214493            0.257971            0.252174\n",
      "7  0.211594  0.214493            0.255072            0.249275\n",
      "8  0.214493  0.217391            0.275362            0.275362\n",
      "9  0.223188  0.214493            0.263768            0.266667\n",
      "   et score  rf score  corrupted et score  corrupted rf score\n",
      "0  0.228986  0.255072            0.220290            0.223188\n",
      "1  0.240580  0.252174            0.289855            0.292754\n",
      "2  0.231884  0.249275            0.260870            0.278261\n",
      "3  0.228986  0.255072            0.252174            0.246377\n",
      "4  0.237681  0.246377            0.240580            0.255072\n",
      "5  0.217391  0.246377            0.269565            0.272464\n",
      "6  0.226087  0.249275            0.223188            0.243478\n",
      "7  0.226087  0.249275            0.286957            0.292754\n",
      "8  0.237681  0.249275            0.272464            0.284058\n",
      "9  0.237681  0.252174            0.243478            0.237681\n",
      "   et score  rf score  corrupted et score  corrupted rf score\n",
      "0  0.298551  0.272464            0.243478            0.249275\n",
      "1  0.286957  0.278261            0.243478            0.252174\n",
      "2  0.304348  0.275362            0.327536            0.321739\n",
      "3  0.295652  0.269565            0.281159            0.284058\n",
      "4  0.301449  0.272464            0.260870            0.260870\n",
      "5  0.289855  0.278261            0.255072            0.263768\n",
      "6  0.295652  0.269565            0.263768            0.249275\n",
      "7  0.289855  0.272464            0.272464            0.272464\n",
      "8  0.289855  0.272464            0.272464            0.289855\n",
      "9  0.286957  0.278261            0.252174            0.263768\n",
      "   et score  rf score  corrupted et score  corrupted rf score\n",
      "0  0.191304  0.191304            0.240580            0.214493\n",
      "1  0.191304  0.202899            0.266667            0.278261\n",
      "2  0.191304  0.191304            0.304348            0.269565\n",
      "3  0.197101  0.188406            0.301449            0.310145\n",
      "4  0.191304  0.202899            0.295652            0.263768\n",
      "5  0.197101  0.176812            0.289855            0.281159\n",
      "6  0.191304  0.220290            0.327536            0.281159\n",
      "7  0.194203  0.226087            0.249275            0.257971\n",
      "8  0.194203  0.228986            0.301449            0.307246\n",
      "9  0.191304  0.231884            0.257971            0.292754\n",
      "   et score  rf score  corrupted et score  corrupted rf score\n",
      "0  0.263768  0.255072            0.220290            0.223188\n",
      "1  0.266667  0.257971            0.231884            0.231884\n",
      "2  0.260870  0.257971            0.263768            0.257971\n",
      "3  0.246377  0.255072            0.318841            0.321739\n",
      "4  0.260870  0.255072            0.255072            0.257971\n",
      "5  0.257971  0.255072            0.168116            0.162319\n",
      "6  0.260870  0.257971            0.252174            0.252174\n",
      "7  0.260870  0.255072            0.269565            0.257971\n",
      "8  0.260870  0.255072            0.275362            0.278261\n",
      "9  0.252174  0.257971            0.327536            0.327536\n",
      "   et score  rf score  corrupted et score  corrupted rf score\n",
      "0  0.240580  0.237681            0.252174            0.260870\n",
      "1  0.240580  0.246377            0.269565            0.275362\n",
      "2  0.237681  0.255072            0.275362            0.292754\n",
      "3  0.240580  0.255072            0.246377            0.266667\n",
      "4  0.237681  0.246377            0.246377            0.234783\n",
      "5  0.249275  0.249275            0.240580            0.246377\n",
      "6  0.237681  0.249275            0.257971            0.272464\n",
      "7  0.240580  0.269565            0.243478            0.237681\n",
      "8  0.243478  0.260870            0.284058            0.298551\n",
      "9  0.234783  0.266667            0.281159            0.263768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>feature</th>\n",
       "      <th>p_value</th>\n",
       "      <th>best_model</th>\n",
       "      <th>scoring difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.214493</td>\n",
       "      <td>conversation dur</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>rf</td>\n",
       "      <td>-0.037971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246377</td>\n",
       "      <td>darkness dur</td>\n",
       "      <td>0.069551</td>\n",
       "      <td>rf</td>\n",
       "      <td>-0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.269565</td>\n",
       "      <td>activity dur</td>\n",
       "      <td>0.332365</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.223188</td>\n",
       "      <td>location changes</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>rf</td>\n",
       "      <td>-0.069565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.255072</td>\n",
       "      <td>location variance</td>\n",
       "      <td>0.477291</td>\n",
       "      <td>rf</td>\n",
       "      <td>-0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.272464</td>\n",
       "      <td>bluetooth colocations</td>\n",
       "      <td>0.072685</td>\n",
       "      <td>rf</td>\n",
       "      <td>-0.011304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy                feature   p_value best_model  scoring difference\n",
       "0  0.214493       conversation dur  0.000007         rf           -0.037971\n",
       "1  0.246377           darkness dur  0.069551         rf           -0.012174\n",
       "2  0.269565           activity dur  0.332365         rf            0.003188\n",
       "3  0.223188       location changes  0.000002         rf           -0.069565\n",
       "4  0.255072      location variance  0.477291         rf           -0.000870\n",
       "5  0.272464  bluetooth colocations  0.072685         rf           -0.011304"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ema_data/PAM, before = False.csv')\n",
    "data = data[data['uid'] == 'u59']\n",
    "data = data[data['window'] == 10]\n",
    "\n",
    "feature_names = ['conversation dur', 'darkness dur', 'activity dur',\n",
    "                 'location changes', 'location variance', 'bluetooth colocations']\n",
    "\n",
    "accuracy_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for feature_name in feature_names: \n",
    "    \n",
    "    #print(feature_name)\n",
    "    features = data[feature_name].values\n",
    "    features = features.reshape(-1, 1)\n",
    "    target = data['picture_idx'].values\n",
    "    \n",
    "    accuracy, conf = tscv_smote(features, target, RandomForestClassifier())\n",
    "    #print(conf)\n",
    "    p_value, scores_df, best_model = validate_model(features, target)\n",
    "    diff = scores_df['{} score'.format(best_model)].mean() - scores_df['corrupted {} score'.format(best_model)].mean()\n",
    "\n",
    "    \"\"\"X_train, X_test, y_train, y_test = train_test_split(features, target, random_state = 0)\n",
    "\n",
    "    sm = SMOTE(sampling_strategy='not majority', random_state = 0)\n",
    "    X_new_train, y_new_train = sm.fit_resample(X_train, y_train)\"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\"rf.fit(X_train, y_train)\n",
    "    print('accuracy without SMOTE: {:.4f}'.format(rf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, rf.predict(X_test), labels=[1,2,3,4]))\"\"\"\n",
    "    \n",
    "    \"\"\"rf = RandomForestClassifier()\n",
    "    rf.fit(X_new_train, y_new_train)\n",
    "    accuracy = rf.score(X_test, y_test)\n",
    "    print('accuracy with SMOTE: {:.4f}'.format(accuracy))\n",
    "    print(confusion_matrix(y_test, rf.predict(X_test), labels=[1,2,3,4]))\"\"\"\n",
    "    \n",
    "    \n",
    "    accuracy_df = accuracy_df.append(pd.DataFrame({'accuracy': [accuracy], 'feature': feature_name, 'p_value': p_value, \n",
    "                                                  'best_model': best_model, 'scoring difference': diff}), ignore_index = True)\n",
    "    \n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(feature_list, data, target_column, confusion_matrices = False): \n",
    "    \n",
    "    accuracy_df = pd.DataFrame()\n",
    "    \n",
    "    for feature_name in feature_list: \n",
    "    \n",
    "        features = data[feature_name].values\n",
    "        \n",
    "        if len(features.shape) == 1: \n",
    "            features = features.reshape(-1, 1)\n",
    "            \n",
    "        target = data[target_column].values\n",
    "\n",
    "        accuracy, conf = tscv_smote(features, target, RandomForestClassifier())\n",
    "        \n",
    "        if confusion_matrices is True: \n",
    "            print(conf)\n",
    "            \n",
    "        p_value, scores_df, best_model = validate_model(features, target)\n",
    "        diff = scores_df['{} score'.format(best_model)].mean() - scores_df['corrupted {} score'.format(best_model)].mean()\n",
    "\n",
    "        accuracy_df = accuracy_df.append(pd.DataFrame({'accuracy': [accuracy], 'feature': [feature_name], 'p_value': p_value, \n",
    "                                                      'best_model': best_model, 'scoring differencevs randomized': diff}), \n",
    "                                         ignore_index = True)\n",
    "    \n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darkness dur', 'activity dur']\n",
      "       Unnamed: 0            resp_time  picture_idx  location changes  \\\n",
      "41508          42  2013-03-27 03:46:23            4              49.0   \n",
      "41509          41  2013-03-27 04:11:27            2              59.0   \n",
      "41510          40  2013-03-27 04:40:53            3              59.0   \n",
      "41511          48  2013-03-27 09:39:42            4             105.0   \n",
      "41512          45  2013-03-27 16:24:23            2             119.0   \n",
      "...           ...                  ...          ...               ...   \n",
      "41920         417  2013-05-31 09:55:34            4             131.0   \n",
      "41921         414  2013-05-31 18:07:45            4              62.0   \n",
      "41922         415  2013-05-31 20:00:38            1              62.0   \n",
      "41923         419  2013-06-01 00:23:29            4              62.0   \n",
      "41924         421  2013-06-01 04:03:15            2              20.0   \n",
      "\n",
      "       activity dur  conversation dur  darkness dur  bluetooth colocations  \\\n",
      "41508        1924.0             192.0       20241.0                     27   \n",
      "41509        2664.0             192.0       20590.0                     29   \n",
      "41510        2863.0             332.0       22356.0                     40   \n",
      "41511        6508.0           10267.0       32598.0                     79   \n",
      "41512        9521.0           20283.0       33857.0                     92   \n",
      "...             ...               ...           ...                    ...   \n",
      "41920        8667.0           23643.0       25177.0                    115   \n",
      "41921       15499.0           21269.0       27237.0                    111   \n",
      "41922       15499.0           21269.0       20464.0                    100   \n",
      "41923       15499.0           21016.0        4958.0                     73   \n",
      "41924       11986.0           14364.0        4958.0                     56   \n",
      "\n",
      "       location variance  deadlines  ...  Saturday  Sunday  day  evening  \\\n",
      "41508         -12.387333          0  ...         0       0    0        0   \n",
      "41509         -12.048155          0  ...         0       0    0        0   \n",
      "41510         -11.669922          0  ...         0       0    0        0   \n",
      "41511         -11.618961          0  ...         0       0    0        0   \n",
      "41512         -11.754101          0  ...         0       0    1        0   \n",
      "...                  ...        ...  ...       ...     ...  ...      ...   \n",
      "41920          -3.940105          0  ...         0       0    0        0   \n",
      "41921           0.712473          0  ...         0       0    0        1   \n",
      "41922           0.747187          0  ...         0       0    0        1   \n",
      "41923           0.765483          0  ...         1       0    0        0   \n",
      "41924           0.440019          0  ...         1       0    0        0   \n",
      "\n",
      "       night  pre midterm  in midterm  post midterm  uid  window  \n",
      "41508      1            1           0             0  u59      10  \n",
      "41509      1            1           0             0  u59      10  \n",
      "41510      1            1           0             0  u59      10  \n",
      "41511      1            1           0             0  u59      10  \n",
      "41512      0            1           0             0  u59      10  \n",
      "...      ...          ...         ...           ...  ...     ...  \n",
      "41920      1            0           0             1  u59      10  \n",
      "41921      0            0           0             1  u59      10  \n",
      "41922      0            0           0             1  u59      10  \n",
      "41923      1            0           0             1  u59      10  \n",
      "41924      1            0           0             1  u59      10  \n",
      "\n",
      "[417 rows x 25 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>feature</th>\n",
       "      <th>p_value</th>\n",
       "      <th>best_model</th>\n",
       "      <th>scoring differencevs randomized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292754</td>\n",
       "      <td>[darkness dur, activity dur]</td>\n",
       "      <td>0.026966</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.015362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy                       feature   p_value best_model  \\\n",
       "0  0.292754  [darkness dur, activity dur]  0.026966         rf   \n",
       "\n",
       "   scoring differencevs randomized  \n",
       "0                         0.015362  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [['darkness dur', 'activity dur']]\n",
    "\n",
    "data = pd.read_csv('ema_data/PAM, before = False.csv')\n",
    "data = data[data['uid'] == 'u59']\n",
    "data = data[data['window'] == 10]\n",
    "\n",
    "evaluate_features(feature_names, data, 'picture_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tscv_smote(features, target, model):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    avg_score = 0\n",
    "    \n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "        \n",
    "    for train_index, test_index in tscv.split(features): \n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        sm = SMOTE(sampling_strategy='not majority', k_neighbors=2, random_state = 0)\n",
    "        X_new_train, y_new_train = sm.fit_resample(X_train, y_train)\n",
    "       \n",
    "        model.fit(X_new_train, y_new_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        #print(\"train indices: {}, test indices: {}, score: {:.2f}\".format(train_index, test_index, score))\n",
    "        #print(\"predictions: {}, actual: {}\".format(model.predict(X_test), target[test_index]))\n",
    "        avg_score += score/5\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        labels.extend(y_test)\n",
    "        predictions.extend(y_pred)\n",
    "        \n",
    "        x = confusion_matrix(labels, predictions, labels=[1,2,3,4])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for i in range(x.shape[0]): \n",
    "        if i == 0: \n",
    "            df = pd.DataFrame({i+1: x[:,i]})\n",
    "        else: \n",
    "            df = df.join(pd.DataFrame({i+1: x[:,i]}))\n",
    "    df.index = [1, 2, 3, 4]\n",
    "            \n",
    "    return avg_score, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(features, target):\n",
    "    \n",
    "    #data = ema_intervals_data(uid, window, ema_name, desired_column)\n",
    "    \n",
    "    extra_trees = ExtraTreesClassifier()\n",
    "    random_forest = RandomForestClassifier()\n",
    "    \n",
    "    et_scores = []\n",
    "    rf_scores = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "   \n",
    "    for i in range(10): \n",
    "    \n",
    "        et_scores.append(tscv_smote(features, target, extra_trees)[0])\n",
    "        rf_scores.append(tscv_smote(features, target, random_forest)[0])\n",
    "                         \n",
    "    cor_et_scores = []\n",
    "    cor_rf_scores = []\n",
    "    \n",
    "    for i in range(10): \n",
    "        \n",
    "        np.random.shuffle(target)\n",
    "    \n",
    "        cor_et_scores.append(tscv_smote(features, target, extra_trees)[0])\n",
    "        cor_rf_scores.append(tscv_smote(features, target, random_forest)[0])\n",
    "    \n",
    "\n",
    "    scores_df = pd.DataFrame({'et score': et_scores, \n",
    "                              'rf score': rf_scores, \n",
    "                              'corrupted et score': cor_et_scores, \n",
    "                              'corrupted rf score': cor_rf_scores, \n",
    "                              })\n",
    "    \n",
    "    rf_p_value = scipy.stats.ttest_ind(scores_df['rf score'], scores_df['corrupted rf score'])[1]\n",
    "    et_p_value = scipy.stats.ttest_ind(scores_df['et score'], scores_df['corrupted et score'])[1]\n",
    "    \n",
    "    #return (1, 2, 3)\n",
    "    \n",
    "    if rf_p_value >= et_p_value: \n",
    "        return rf_p_value/2, scores_df[['rf score', 'corrupted rf score']], 'rf'\n",
    "    else: \n",
    "        return et_p_value/2, scores_df[['et score', 'corrupted et score']], 'et'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>picture_idx</th>\n",
       "      <th>location changes</th>\n",
       "      <th>activity dur</th>\n",
       "      <th>conversation dur</th>\n",
       "      <th>darkness dur</th>\n",
       "      <th>bluetooth colocations</th>\n",
       "      <th>location variance</th>\n",
       "      <th>deadlines</th>\n",
       "      <th>...</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>pre midterm</th>\n",
       "      <th>in midterm</th>\n",
       "      <th>post midterm</th>\n",
       "      <th>uid</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39844</th>\n",
       "      <td>42</td>\n",
       "      <td>2013-03-27 03:46:23</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-16.377472</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39845</th>\n",
       "      <td>41</td>\n",
       "      <td>2013-03-27 04:11:27</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-16.567959</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39846</th>\n",
       "      <td>40</td>\n",
       "      <td>2013-03-27 04:40:53</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>8</td>\n",
       "      <td>-16.854579</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39847</th>\n",
       "      <td>48</td>\n",
       "      <td>2013-03-27 09:39:42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>12</td>\n",
       "      <td>-21.555614</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39848</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-03-27 16:24:23</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3358.0</td>\n",
       "      <td>6419.0</td>\n",
       "      <td>9458.0</td>\n",
       "      <td>38</td>\n",
       "      <td>-11.625681</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40253</th>\n",
       "      <td>418</td>\n",
       "      <td>2013-05-31 04:07:48</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>7234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>-12.396650</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40254</th>\n",
       "      <td>417</td>\n",
       "      <td>2013-05-31 09:55:34</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>25</td>\n",
       "      <td>-21.612672</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40255</th>\n",
       "      <td>414</td>\n",
       "      <td>2013-05-31 18:07:45</td>\n",
       "      <td>4</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7086.0</td>\n",
       "      <td>9443.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.809354</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40256</th>\n",
       "      <td>415</td>\n",
       "      <td>2013-05-31 20:00:38</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5436.0</td>\n",
       "      <td>7469.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.254406</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40257</th>\n",
       "      <td>419</td>\n",
       "      <td>2013-06-01 00:23:29</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5731.0</td>\n",
       "      <td>4043.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>29</td>\n",
       "      <td>-3.612579</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>u59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            resp_time  picture_idx  location changes  \\\n",
       "39844          42  2013-03-27 03:46:23            4               8.0   \n",
       "39845          41  2013-03-27 04:11:27            2               8.0   \n",
       "39846          40  2013-03-27 04:40:53            3               8.0   \n",
       "39847          48  2013-03-27 09:39:42            4               0.0   \n",
       "39848          45  2013-03-27 16:24:23            2              29.0   \n",
       "...           ...                  ...          ...               ...   \n",
       "40253         418  2013-05-31 04:07:48            2              27.0   \n",
       "40254         417  2013-05-31 09:55:34            4               0.0   \n",
       "40255         414  2013-05-31 18:07:45            4              49.0   \n",
       "40256         415  2013-05-31 20:00:38            1              20.0   \n",
       "40257         419  2013-06-01 00:23:29            4              -1.0   \n",
       "\n",
       "       activity dur  conversation dur  darkness dur  bluetooth colocations  \\\n",
       "39844          13.0             192.0           0.0                      5   \n",
       "39845          15.0             192.0           0.0                      6   \n",
       "39846          19.0             192.0        1203.0                      8   \n",
       "39847           0.0               0.0       14400.0                     12   \n",
       "39848        3358.0            6419.0        9458.0                     38   \n",
       "...             ...               ...           ...                    ...   \n",
       "40253         920.0            7234.0           0.0                     25   \n",
       "40254           0.0               0.0       14400.0                     25   \n",
       "40255        7086.0            9443.0           0.0                     14   \n",
       "40256        5436.0            7469.0           0.0                     25   \n",
       "40257        5731.0            4043.0        3350.0                     29   \n",
       "\n",
       "       location variance  deadlines  ...  Saturday  Sunday  day  evening  \\\n",
       "39844         -16.377472          0  ...         0       0    0        0   \n",
       "39845         -16.567959          0  ...         0       0    0        0   \n",
       "39846         -16.854579          0  ...         0       0    0        0   \n",
       "39847         -21.555614          0  ...         0       0    0        0   \n",
       "39848         -11.625681          0  ...         0       0    1        0   \n",
       "...                  ...        ...  ...       ...     ...  ...      ...   \n",
       "40253         -12.396650          0  ...         0       0    0        0   \n",
       "40254         -21.612672          0  ...         0       0    0        0   \n",
       "40255          -1.809354          0  ...         0       0    0        1   \n",
       "40256          -0.254406          0  ...         0       0    0        1   \n",
       "40257          -3.612579          0  ...         1       0    0        0   \n",
       "\n",
       "       night  pre midterm  in midterm  post midterm  uid  window  \n",
       "39844      1            1           0             0  u59       2  \n",
       "39845      1            1           0             0  u59       2  \n",
       "39846      1            1           0             0  u59       2  \n",
       "39847      1            1           0             0  u59       2  \n",
       "39848      0            1           0             0  u59       2  \n",
       "...      ...          ...         ...           ...  ...     ...  \n",
       "40253      1            0           0             1  u59       2  \n",
       "40254      1            0           0             1  u59       2  \n",
       "40255      0            0           0             1  u59       2  \n",
       "40256      0            0           0             1  u59       2  \n",
       "40257      1            0           0             1  u59       2  \n",
       "\n",
       "[414 rows x 25 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ema_data/PAM, before = False.csv')\n",
    "data = data[data['uid'] == 'u59']\n",
    "data = data[data['window'] == 2]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loneliness Scale Survey Data Treatment. Also adapted from the other student's notebook\n",
    "\n",
    "def lonely_analysis(lonely):\n",
    "    \"\"\"\n",
    "    Consolidates the  block of code necessary to generate the Loneliness survey visualizations for\n",
    "    added modularity of notebook. Running it on the raw Loneliness data prepares the graphs related\n",
    "    to this piece of the dataset.\n",
    "    \n",
    "    @param: lonely â€“Â raw data for Loneliness survey, obtained by using pandas' read_csv method\n",
    "    \n",
    "    returns: returns remodeled dataframes for the pre- and post-study halves of the original dataframe\n",
    "             as a tuple for integrated visualizations with other studies. \n",
    "             Prepares graphs for Loneliness survey visualization.\n",
    "             plt.show() should be run outside of function call for visualization\n",
    "    \"\"\"\n",
    "    pre_lonely = lonely[lonely.type == 'pre'].drop('type', axis=1)\n",
    "    post_lonely = lonely[lonely.type == 'post'].drop('type', axis=1)\n",
    "\n",
    "    def remodel_columns_lonely(data):\n",
    "        \"\"\"\n",
    "        Replaces the wordy columns for indices q1-q20. Since LonelinessScale\n",
    "        is standardized, all questions follow the same order and\n",
    "        can be referred to by indices for simplification.\n",
    "\n",
    "        @param: data â€“ dataframe containing PSS survey data\n",
    "\n",
    "        returns: modified dataframe with q1-q20 indexed columns\n",
    "        \"\"\"\n",
    "        index_dict = {}\n",
    "        for ind in range(data.shape[1]):\n",
    "            index_dict[data.columns[ind]] = f\"q{ind + 1}\"\n",
    "\n",
    "        data = data.rename(columns=index_dict)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def numerify_data_lonely(entry):\n",
    "        \"\"\"\n",
    "        Replaces string response for corresponding value 1-4.\n",
    "\n",
    "        @param: dataframe entry containing LonelinessScale survey answer\n",
    "\n",
    "        returns: value 1-4 replacing str answer\n",
    "        \"\"\"\n",
    "        if entry == 'Never':\n",
    "            return 1\n",
    "        if entry == 'Rarely':\n",
    "            return 2\n",
    "        if entry == 'Sometimes':\n",
    "            return 3\n",
    "        if entry == 'Often':\n",
    "            return 4\n",
    "        return entry\n",
    "\n",
    "    def remodel_data_lonely(data):\n",
    "        \"\"\"\n",
    "        Combines functionalities of remodel_columns_lonely and numerify_data_lonely\n",
    "        for each entry, offering a dataframe more suitable for analysis. Also\n",
    "        adds the test score for each student as a new column.\n",
    "\n",
    "        @param: data â€“ dataframe containing PSS survey data\n",
    "\n",
    "        returns: modified dataframe with q1-q20 indexed columns and values 1-4\n",
    "        replacing original str answers in q1-q20, with new column 'score'\n",
    "        with each student's test score\n",
    "        \"\"\"\n",
    "        data = remodel_columns_lonely(data)\n",
    "        data = data.applymap(numerify_data_lonely)\n",
    "        for question in {'q1', 'q5', 'q6', 'q9', 'q10',\n",
    "                         'q15', 'q16', 'q19', 'q20'}:\n",
    "            data[question] = data[question].apply(lambda x: 5 - x)\n",
    "        data['score'] = data.sum(axis=1, numeric_only=True)\n",
    "        data['id'] = data.index\n",
    "        return data\n",
    "\n",
    "    pre_lonely_m = remodel_data_lonely(pre_lonely)\n",
    "    post_lonely_m = remodel_data_lonely(post_lonely)\n",
    "    \n",
    "    return pre_lonely_m, post_lonely_m\n",
    "\n",
    "###All the following survey processing code is adapted from the notebook \"Survey Dataset V2\" \n",
    "### which was written by another student\n",
    "\n",
    "### This function processes perceived stress scale\n",
    "\n",
    "def pss_analysis(pss_survey):\n",
    "    \"\"\"\n",
    "    Consolidates the  block of code necessary to generate the PSS survey visualizations for\n",
    "    added modularity of notebook. Running it on the raw PSS data prepares the graphs related\n",
    "    to this piece of the dataset.\n",
    "    \n",
    "    @param: pss_survey â€“Â raw data for PSS survey, obtained by using pandas' read_csv method\n",
    "    \n",
    "    returns: returns remodeled dataframes for the pre- and post-study halves of the original dataframe\n",
    "             as a tuple for integrated visualizations with other studies. \n",
    "             Prepares graphs for PSS survey visualization.\n",
    "             plt.show() should be run outside of function call for visualization\n",
    "    \"\"\"\n",
    "    pre_pss = pss_survey[pss_survey.type == 'pre'].drop('type', axis=1)\n",
    "    post_pss = pss_survey[pss_survey.type == 'post'].drop('type', axis=1)\n",
    "\n",
    "    def remodel_columns_pss(data):\n",
    "        \"\"\"\n",
    "        Replaces the wordy columns for indices q1-q10. Since PSS\n",
    "        is standardized, all questions follow the same order and\n",
    "        can be referred to by indices for simplification.\n",
    "\n",
    "        @param: data â€“ dataframe containing PSS survey data\n",
    "\n",
    "        returns: modified dataframe with q1-q10 indexed columns\n",
    "        \"\"\"\n",
    "        index_dict = {}\n",
    "        for ind in range(data.shape[1]):\n",
    "            index_dict[data.columns[ind]] = f\"q{ind + 1}\"\n",
    "\n",
    "        data = data.rename(columns=index_dict)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def numerify_data_pss(entry):\n",
    "        \"\"\"\n",
    "        Replaces string response for corresponding value 0-4.\n",
    "\n",
    "        @param: dataframe entry containing PSS survey answer\n",
    "\n",
    "        returns: value 0-4 replacing str answer\n",
    "        \"\"\"\n",
    "        if entry == 'Never':\n",
    "            return 0\n",
    "        if entry == 'Almost never':\n",
    "            return 1\n",
    "        if entry == 'Sometime':\n",
    "            return 2\n",
    "        if entry == 'Fairly often':\n",
    "            return 3\n",
    "        if entry == 'Very often':\n",
    "            return 4\n",
    "        return entry\n",
    "\n",
    "    def remodel_data_pss(data):\n",
    "        \"\"\"\n",
    "        Combines functionalities of remodel_columns_pss and numerify_data_pss\n",
    "        for each entry, offering a dataframe more suitable for analysis. Also\n",
    "        adds the test score for each student as a new column.\n",
    "\n",
    "        @param: data â€“ dataframe containing PSS survey data\n",
    "\n",
    "        returns: modified dataframe with q1-q10 indexed columns and values 0-4\n",
    "        replacing original str answers in q1-q10, with new columns 'score'\n",
    "        with each student's test score\n",
    "        \"\"\"\n",
    "        data = remodel_columns_pss(data)\n",
    "        data = data.applymap(numerify_data_pss)\n",
    "        # Reverse scoring for particular questions\n",
    "        for question in {'q4', 'q5', 'q7', 'q8'}:\n",
    "            data[question] = data[question].apply(lambda x: 4 - x)\n",
    "        data['score'] = data.sum(axis=1, numeric_only=True)\n",
    "        data['id'] = data.index\n",
    "        return data\n",
    "\n",
    "    pre_pss_m = remodel_data_pss(pre_pss)\n",
    "    post_pss_m = remodel_data_pss(post_pss)\n",
    "\n",
    "    return pre_pss_m, post_pss_m\n",
    "\n",
    "### PHQ-9 Survey Data Treatment  \n",
    "\n",
    "def phq_analysis(phq_survey):\n",
    "    \"\"\"\n",
    "    Consolidates the  block of code necessary to generate the PHQ-9 survey visualizations for\n",
    "    added modularity of notebook. Running it on the raw PHQ-9 data prepares the graphs related\n",
    "    to this piece of the dataset.\n",
    "    \n",
    "    @param: phq_survey â€“Â raw data for PHQ-9 survey, obtained by using pandas' read_csv method\n",
    "    \n",
    "    returns: returns remodeled dataframes for the pre- and post-study halves of the\n",
    "             original dataframe as a tuple for integrated visualizations with other studies. \n",
    "    \"\"\"\n",
    "    pre_phq = phq_survey[phq_survey.type == 'pre'].drop('type', axis=1)\n",
    "    post_phq = phq_survey[phq_survey.type == 'post'].drop('type', axis=1)\n",
    "\n",
    "    def remodel_columns_phq(data):\n",
    "        \"\"\"\n",
    "        Replaces the wordy columns for indices q1-q10. Since PHQ-9\n",
    "        is standardized, all questions follow the same order and\n",
    "        can be referred to by indices for simplification.\n",
    "\n",
    "        @param: data â€“ dataframe containing PHQ-9 survey data\n",
    "\n",
    "        returns: modified dataframe with q1-q10 indexed columns\n",
    "        \"\"\"\n",
    "        index_dict = {}\n",
    "        for ind in range(data.shape[1]):\n",
    "            index_dict[data.columns[ind]] = f\"q{ind + 1}\"\n",
    "\n",
    "        data = data.rename(columns=index_dict)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def numerify_data_phq(entry):\n",
    "        \"\"\"\n",
    "        Replaces string response for corresponding value 0-3.\n",
    "\n",
    "        @param: entry â€“Â dataframe entry containing PHQ-9 survey answer\n",
    "\n",
    "        returns: value 0-3 replacing str answer; for q10, simply returns same\n",
    "        str entry (column q10 is not graded)\n",
    "        \"\"\"\n",
    "        if entry == 'Not at all':\n",
    "            return 0\n",
    "        if entry == 'Several days':\n",
    "            return 1\n",
    "        if entry == 'More than half the days':\n",
    "            return 2\n",
    "        if entry == 'Nearly every day':\n",
    "            return 3\n",
    "        return entry\n",
    "\n",
    "    def severity_analysis_phq(score):\n",
    "        \"\"\"\n",
    "        Classifies each student's score according to the PHQ-9 classification standard\n",
    "        \n",
    "        @param: data.score â€“Â 'score' column of dataframe\n",
    "        \n",
    "        returns: new column which can be assigned to new label 'severity_level'\n",
    "        \"\"\"\n",
    "        if score <= 4:\n",
    "            return 'normal'\n",
    "        if score <= 9:\n",
    "            return 'mild'\n",
    "        if score <= 14:\n",
    "            return 'moderate'\n",
    "        if score <= 19:\n",
    "            return 'moderately severe'\n",
    "        return 'severe'\n",
    "\n",
    "    def remodel_data_phq(data):\n",
    "        \"\"\"\n",
    "        Combines functionalities of remodel_columns_phq and numerify_data_phq\n",
    "        for each entry, offering a dataframe more suitable for analysis. Also\n",
    "        adds the test score for each student as a new column.\n",
    "\n",
    "        @param: data â€“ dataframe containing PHQ-9 survey data\n",
    "\n",
    "        returns: modified dataframe with q1-q10 indexed columns and values 0-3\n",
    "        replacing original str answers in q1-q9, with new columns 'score' and\n",
    "        'severity_level' with each student's test score and classification.\n",
    "        \"\"\"\n",
    "        data = remodel_columns_phq(data)\n",
    "        data = data.applymap(numerify_data_phq)\n",
    "        data['score'] = data.sum(axis=1, numeric_only=True)\n",
    "        data['severity_level'] = data.score.apply(severity_analysis_phq)\n",
    "        data['id'] = data.index\n",
    "        return data\n",
    "\n",
    "    pre_phq_m = remodel_data_phq(pre_phq)\n",
    "    post_phq_m = remodel_data_phq(post_phq)\n",
    "\n",
    "    return pre_phq_m, post_phq_m\n",
    "\n",
    "pss = pd.read_csv(\"dataset/survey/PerceivedStressScale.csv\", index_col=0)\n",
    "prepss, postpss = pss_analysis(pss)\n",
    "    \n",
    "loneliness = pd.read_csv(\"dataset/survey/LonelinessScale.csv\", index_col=0)\n",
    "prelonely, postlonely = lonely_analysis(loneliness)\n",
    "    \n",
    "phq = pd.read_csv(\"dataset/survey/PHQ-9.csv\", index_col=0)\n",
    "pre_phq, post_phq = phq_analysis(phq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def clustering(uid_list, n_clusters): \n",
    "    \"\"\"\n",
    "    inputs: compiled_features containing survey scores and sensor rankings. \n",
    "    \n",
    "    this function performs agglomerative clustering with 4 groups. \n",
    "    \"\"\"    \n",
    "    pss = pd.read_csv(\"dataset/survey/PerceivedStressScale.csv\", index_col=0)\n",
    "    prepss, postpss = pss_analysis(pss)\n",
    "\n",
    "    loneliness = pd.read_csv(\"dataset/survey/LonelinessScale.csv\", index_col=0)\n",
    "    prelonely, postlonely = lonely_analysis(loneliness)\n",
    "\n",
    "    phq = pd.read_csv(\"dataset/survey/PHQ-9.csv\", index_col=0)\n",
    "    pre_phq, post_phq = phq_analysis(phq)\n",
    "\n",
    "    clustering = AgglomerativeClustering(n_clusters = n_clusters)\n",
    "    \n",
    "    survey_list = [(prepss, 'pss'), (prelonely, 'loneliness'), (pre_phq, 'phq')]\n",
    "    \n",
    "    survey_df = pd.DataFrame({'id':uid_list})\n",
    "    \n",
    "    for survey in survey_list:     \n",
    "        survey_df = survey_df.merge(survey[0][['score', 'id']], on = 'id', how = 'inner')\n",
    "        survey_df = survey_df.rename(columns = {'score': survey[1] + ' score'})\n",
    "        \n",
    "    clustering.fit(survey_df[['pss score', 'loneliness score', 'phq score']])\n",
    "    \n",
    "    survey_df['cluster'] = clustering.labels_\n",
    "    \n",
    "    return survey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pss score</th>\n",
       "      <th>loneliness score</th>\n",
       "      <th>phq score</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u16</td>\n",
       "      <td>24.0</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u19</td>\n",
       "      <td>20.0</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u57</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u58</td>\n",
       "      <td>20.0</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>u59</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  pss score  loneliness score  phq score  cluster\n",
       "0  u10       20.0                64          0        1\n",
       "1  u16       24.0                35          6        0\n",
       "2  u19       20.0                55          5        1\n",
       "3  u57        9.0                44          0        0\n",
       "4  u58       20.0                51          5        1\n",
       "5  u59       18.0                37          5        0"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering(['u10', 'u16', 'u19', 'u57', 'u58', 'u59'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u00',\n",
       " 'u02',\n",
       " 'u04',\n",
       " 'u08',\n",
       " 'u10',\n",
       " 'u12',\n",
       " 'u16',\n",
       " 'u17',\n",
       " 'u19',\n",
       " 'u22',\n",
       " 'u32',\n",
       " 'u33',\n",
       " 'u35',\n",
       " 'u36',\n",
       " 'u43',\n",
       " 'u44',\n",
       " 'u49',\n",
       " 'u51',\n",
       " 'u52',\n",
       " 'u53',\n",
       " 'u57',\n",
       " 'u58',\n",
       " 'u59']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['conversation dur', 'darkness dur', 'activity dur',\n",
    "                 'location changes', 'location variance', 'bluetooth colocations', ['darkness dur', 'activity dur'], \n",
    "                ['conversation dur', 'darkness dur', 'activity dur',\n",
    "                 'location changes', 'location variance', 'bluetooth colocations']]\n",
    "\n",
    "data = pd.read_csv('ema_data/PAM, before = False.csv')\n",
    "#data = data[data['uid'].isin(uid_list)]\n",
    "data = data[data['window'] == 10]\n",
    "\n",
    "evaluate_features(feature_names, data, 'picture_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
